## Car Tracking
This involves 5 steps:
1. Camera Calibration - To recover focal length of the camera and its orientation with respect to the stream of traffic. 
2. Vehicle detection - To understand whether to proceed with our algorithm or not.
3. Construct 3D bounding box - to help calculate dimensions of the car.
4. Identify Vehicle Dimensions - knowing dimensions of the car will help identifying the distance travelled between frames.
5. Calculate Distance and Speed Measurement - This calculates the distance travelled between frames and thereby deduce the speed.

### 1. Camera Calibration - To recover focal length of the camera and its orientation with respect to the stream of traffic. [Reference](https://medusa.fit.vutbr.cz/traffic/data/papers/2014-IEEE-ITS-CameraCalibration.pdf)
- Diamond Space - Maps the whole 2-D projective plane into a finite space referred to as the diamond space
      - [Reference](https://pdfs.semanticscholar.org/fa90/8a206be873c9de34a6997d315ea9f5c31e00.pdf) 
      - Use cascaded transformation to transform each point to a line and then again to a point.
      - (We are not clear on 2 things - Please refer to figure 2 [here](https://pdfs.semanticscholar.org/fa90/8a206be873c9de34a6997d315ea9f5c31e00.pdf) - Not sure why we need the second transformation and how the 4 quadrants in figure 3 translate to the 2-D projective plane)
- First VP Extraction [Reference](https://medusa.fit.vutbr.cz/traffic/data/papers/2014-IEEE-ITS-CameraCalibration.pdf)
       - VP of direction parallel to the movement of the vehicles is considered to be the first VP (**not sure how to find the direction of the movement of the vehicles**)
      - Detect feature points using minimum eigenvalue algorithm.(use cv2.goodFeaturesToTrack())
      - Track using Kanade–Lucas–Tomasi tracker in the subsequent frame
      - Identify points that exhibit movements (these are fragments of vehicle trajectories)
      - Extend such line fragments to pass through the diamond space. This will pass through the diamond space at a point.
      - The point where most the line fragements pass through is the first VP.
- Second VP Extraction [Reference](https://medusa.fit.vutbr.cz/traffic/data/papers/2014-IEEE-ITS-CameraCalibration.pdf)
        - The second VP corresponds to the direction parallel to the road (or the ground plane) and perpendicular to the first direction (**not sure how to find the direction of the movement of the vehicle**)
      - Detect edges on moving vehicles. For every pixel mark the gradient magnitude.
      - In order to detect shadows select an edge whose gradient magnitude is above a certain threshold.
      - Extend those edges to pass through the diamond space.
      - Drop edges passing close to the first VP.
      - The point where most edges pass through the diamond space is the second VP.
- Third VP, Principal Point and Focal Length Extraction [Reference](https://medusa.fit.vutbr.cz/traffic/data/papers/2014-IEEE-ITS-CameraCalibration.pdf)
      - Calculate focal length,f, using 
      f= -\sqrt{-(U - P). (V - P)}
      where U and V are the first two vanishing points and P is the principal point of the image, assumed to be in the middle of the image.
      U = (Ux, Uy), V= (Vx, Vy) and P = (Px, Py)
      - The 3rd VP, W is calculated as
      Uprime = (Ux, Uy, f), Vprime= (Vx, Vy, f) and Pprime = (Px, Py, 0)
      Wprime = (Uprime = Pprime) X (Vprime - Pprime)
    - Camera Calibration from the VPs
      - Transform a world point, [xw, yw, zw, 1] into point [xp, yp, 1] in the image plane.
          λp[xp, yp, 1] = P[xw, yw, zw, 1]
       - The projection matrix P is composed of three matrices.
           - K which contains the focal length of the camera
           - R which contains the rotation matrix, which is the 3 vanishing points
           - T which contains the translation vector, which is the height of the camera from the ground.
### 2. Vechicle detection in all frames - [Reference](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Sochor_GraphFIT_Submission_to_CVPR_2018_paper.pdf)
Faster-RCNN with ResNet101 backbone was used. The detector was trained on UA-DETRAC and COD20K datasets. The detections were merged to tracks using Kalman filter 

- Faster RCNN is made up of fast R-CNN(detector) and region proposer.
- ZFNet (an improvement of AlexNet) or VGG 16 is used for the CNN layer.
- RPN is used to identify the proposed regions.
4 steps to train faster RCNN:
    1. Train RPN, initialized with Imagenet pre-trained model.
    2. Train a separate detection network by fast R-CNN using proposals generated by step-1 RPN, initialized by imagenet pretrained model.
    3. Fix conv layer, fine tune unique layers to RPN, initialized by detector network in step 2.
    4. Fix conv layer, fine tune fc-layers of fast R-CNN.
  
**Input to this step** - Pictures of cars and some other objects.
**Output from this step** - Two outputs for each candidate object, a class label and a bounding-box offset
**Dataset to be used for training** COCO dataset
**My Notes - Testing time on RCNN is relatively slow. Region Proposal Networks are faster (RPN) and they share convolutional layers. Testing time for faster RCNN for one image is 0.2 seconds.
Google's Tensorflow Object Detection API can be used for both the detection and drawing bounding boxes. The authors of the paper talk about training a Faster R-CNN but looks like tensorflow's API may be easier. Reference - [Blog1](https://www.deeplearninganalytics.org/blog/introduction-to-tensorflow-object-detection-api), [GitHub1](https://github.com/priya-dwivedi/Deep-Learning/blob/master/Object_Detection_Tensorflow_API.ipynb)
[Step by Step TensorFlow Object Detection API Tutorial — Part 1: Selecting a Model](https://medium.com/@WuStangDan/step-by-step-tensorflow-object-detection-api-tutorial-part-1-selecting-a-model-a02b6aabe39e)**

### 3. Construction of 3D Bounding Boxes [Reference](http://www.bmva.org/bmvc/2014/files/paper013.pdf)
- Extract Vehicle Silhouettes [Reference](http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf)
  - C++ code is available [here](https://github.com/yangyangHu/GMM-For-Tracking)
  - Repeat the below over a time series of pixel values (the underlying assumption is that anything that moves produces more variance than a static object)
       - Model the values of each pixel as a Gaussian distribution. 
       - Identify background colors based on the persistence and the variance of each of the Gaussians of the mixture. 
        - Pixel values that do not fit the background distributions are considered foreground until there is a Gaussian that includes them with sufficient, consistent evidence supporting it.
  - Detect foreground blobs and remove shadows [Reference](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.1244&rep=rep1&type=pdf)
      - C++ code is available [here](https://github.com/goossaert/computer-vision/tree/master/lambertain)
      - Evaluate brightness and chromaticity for each pixel. 
      - Compute a statistical model for that particular pixel. 
      - Using a detection rate, detect which values of brightness and chromaticity can be used as threshold to select or reject novel pixel intensities.
    - Construct 3D Bounding Box [Reference - Figure 3](http://www.bmva.org/bmvc/2014/files/paper013.pdf)
      - For each blob construct the bounding box in the following order:
        - Assume the width of the car is marked as AB.
          - Point A is marked as the intersection of the first and the second VP (**when there are multiple lines passing through not sure how the correct one can be selected**)
          - Point B is marked as the intersection of the second and the third VP.
        - Assume the length of the car is marked as AC
          - Point C is marked as the intersection of the second and the third VP (**not sure how to mark B and C because both intersect second and third VP)
          - Assume the height of the car is marked as AD
            - Point D is marked as the intersection of the of the first and second VP.
        - Similarly points E, F, G & H are marked.

**Input to this step** - Pictures of cars detected in step 2
**Output from this step** - Pictures of cars with bounding box around each cars identified in the input

### 4. Identify Vehicle Dimensions
- Please refer to Fig 5 and 6 for this calculation in [this](http://www.bmva.org/bmvc/2014/files/paper013.pdf) paper.

**Input to this step** - Pictures of cars with bounding box from step 3
**Output from this step** - Dimensions of each cars identified in the input
### 5. Calculate Distance and Speed Measurement [Reference ](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Sochor_GraphFIT_Submission_to_CVPR_2018_paper.pdf)
- Estimate Scale 
  - Use Google Earth to measure the real word distance of 2 points. 
  - Take around 40 such measurements to reduce errors.
   - Divide those measuremets, M, into 2 groups. 
     - The first group Mu contains measurements in the direction of the first vanishing point
     - The second group Mv is computed as Mv = M \ Mu.
    - Refine Mv using the equation 1 in [this](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Sochor_GraphFIT_Submission_to_CVPR_2018_paper.pdf) paper.
     - Calculate scale, lambda, using equation 14 mentioned in [this](https://arxiv.org/pdf/1702.06451.pdf) paper.
    - Calculate Speed
      - Form a 2x2 orthogonal grid on the plane. Using this the distance between 2 points can be measured.
      - Identify the vehicle of interest and its grid point.
      - Move to another videoframe and identify the same vehicle and its grid point.
      - Knowing these 2 points and the framerate the speed can be calculated.

**Input to this step** - Pictures of cars with dimensions from step 4
**Output from this step** - Speed of each car whose dimensions where provided.